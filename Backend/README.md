# Backend Setup

The FastAPI backend stores user data and handles chat sessions.

## Dependencies

```bash
python -m pip install fastapi uvicorn pydantic databases aiosqlite aiohttp
```

`google-cloud-translate` is still optional if you need the old translation
endpoint, but it isn't required for the AI response layer below.

> `google-cloud-translate` is optional; the code falls back to a mock translation if the library
> is not available.

## Running

```bash
cd Backend
uvicorn main:app --host 127.0.0.1 --port 8000 --reload
```

## Translation & AI setup

### Google Translate (optional)

To keep the existing `/translate` endpoint functionality:

1. Create a Google Cloud project and enable the Translate API.
2. Download a service account JSON key and set the
   `GOOGLE_APPLICATION_CREDENTIALS` environment variable to its path:

   ```powershell
   $env:GOOGLE_APPLICATION_CREDENTIALS="C:\path\to\your\key.json"
   ```

3. Restart the backend; the `/translate` endpoint will then call the
   Google library and return actual translations.

If the library is missing or credentials are not configured, the endpoint
will respond with a formatted string so the frontend continues working.

### Hugging Face AI (free)

The chat responses are now generated by a model hosted on Hugging Face.

1. **Sign up** for a free account at https://huggingface.co/ and obtain an API
   key from your user settings.
2. Export it as an environment variable before starting the server:

   ```powershell
   $env:HF_API_KEY="your_key_here"
   ```

   or add the same line to a `.env` file loaded by your shell.

3. The backend will call the `gpt2` textâ€‘generation model, then automatically
   translate replies into the conversation's language.  No additional code
   changes are required in the frontend.

Because the HF inference service provides a generous free tier, the
entire AI layer can run at no cost for light development use.  You can
switch to a larger or local model by editing `main.py` if needed.
